{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cee3e7bb-832f-4957-a5e7-b737b11c49d7",
   "metadata": {},
   "source": [
    "AIzaSyAmWmCdGzKRM1TS-YF2DzBZydD0wz0TfBA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca34325-6e35-4010-b310-c3e287552594",
   "metadata": {},
   "source": [
    "# Project: Academic RAG Agent - Your Personal Course Assistant\n",
    "\n",
    "This notebook builds a complete AI agent to answer questions about your university coursework. This project fulfills all mandatory requirements of the internship assignment (`data_science.pdf`):\n",
    "\n",
    "1.  **Automates a Manual Task:** Automates the \"daily task\" of searching through multiple PDF notes and textbooks to find answers, formulas, and definitions.\n",
    "2.  **Uses a Fine-Tuned Model:** We will fine-tune (and simulate) a \"Generator\" model to act as an expert tutor that answers questions based *only* on the provided documents.\n",
    "3.  **Includes RAG:** This is an external integration (RAG, or Retrieval-Augmented Generation) to find the relevant information.\n",
    "4.  **Implements Evaluation:** We will build an evaluation suite (an \"LLM-as-a-Judge\") to measure the quality and reliability of our agent's answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743bca83-29d5-4bd5-af66-1f6054494312",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries\n",
    "\n",
    "We will use:\n",
    "* `google-generativeai`: For the LLM (Generator and Evaluator).\n",
    "* `langchain`, `langchain-community`, `langchain-text-splitters`: To easily load and split the PDFs.\n",
    "* `pymupdf`: A powerful PDF parsing library needed by LangChain.\n",
    "* `faiss-cpu`: A local vector database to store and search our notes.\n",
    "* `sentence-transformers`: To create the embeddings (vectors) for our documents.\n",
    "* `langchain_google_genai`: To integrate Gemini models with LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ff54379-c63a-4fe3-80e8-d5b9c65a1943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.3.2)\n",
      "Collecting google-generativeai\n",
      "  Using cached google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: langchain-core in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: langchain-google-genai in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.0.0)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
      "  Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-generativeai) (2.28.1)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-generativeai) (2.179.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-generativeai) (2.42.1)\n",
      "Requirement already satisfied: protobuf in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-generativeai) (4.25.8)\n",
      "Requirement already satisfied: pydantic in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-generativeai) (2.12.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-generativeai) (4.15.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core->google-generativeai) (1.71.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.5)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.62.3)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (6.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.10.5)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core) (0.4.39)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core) (9.1.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic->google-generativeai) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic->google-generativeai) (0.4.2)\n",
      "INFO: pip is looking at multiple versions of langchain-google-genai to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-google-genai\n",
      "  Using cached langchain_google_genai-3.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Using cached langchain_google_genai-2.1.12-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Using cached langchain_google_genai-2.1.11-py3-none-any.whl.metadata (6.7 kB)\n",
      "  Using cached langchain_google_genai-2.1.10-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-google-genai) (1.2.0)\n",
      "INFO: pip is still looking at multiple versions of langchain-google-genai to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached langchain_google_genai-2.1.9-py3-none-any.whl.metadata (7.2 kB)\n",
      "  Using cached langchain_google_genai-2.1.8-py3-none-any.whl.metadata (7.0 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached langchain_google_genai-2.1.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Using cached langchain_google_genai-2.1.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Using cached langchain_google_genai-2.1.5-py3-none-any.whl.metadata (5.2 kB)\n",
      "  Using cached langchain_google_genai-2.1.4-py3-none-any.whl.metadata (5.2 kB)\n",
      "  Using cached langchain_google_genai-2.1.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "  Using cached langchain_google_genai-2.1.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "  Using cached langchain_google_genai-2.1.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "  Using cached langchain_google_genai-2.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "  Using cached langchain_google_genai-2.0.11-py3-none-any.whl.metadata (3.6 kB)\n",
      "  Using cached langchain_google_genai-2.0.10-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting langchain-core\n",
      "  Using cached langchain_core-0.3.79-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.3.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\kanis\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Using cached google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
      "Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "Using cached langchain_google_genai-2.0.10-py3-none-any.whl (41 kB)\n",
      "Using cached langchain_core-0.3.79-py3-none-any.whl (449 kB)\n",
      "Installing collected packages: langchain-core, google-ai-generativelanguage, google-generativeai, langchain-google-genai\n",
      "\n",
      "  Attempting uninstall: langchain-core\n",
      "\n",
      "    Found existing installation: langchain-core 1.0.2\n",
      "\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "    Uninstalling langchain-core-1.0.2:\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "      Successfully uninstalled langchain-core-1.0.2\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "  Attempting uninstall: google-ai-generativelanguage\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "    Found existing installation: google-ai-generativelanguage 0.9.0\n",
      "   ---------------------------------------- 0/4 [langchain-core]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "    Uninstalling google-ai-generativelanguage-0.9.0:\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "      Successfully uninstalled google-ai-generativelanguage-0.9.0\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "  Attempting uninstall: google-generativeai\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "    Found existing installation: google-generativeai 0.3.2\n",
      "   ---------- ----------------------------- 1/4 [google-ai-generativelanguage]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "    Uninstalling google-generativeai-0.3.2:\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "      Successfully uninstalled google-generativeai-0.3.2\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "  Attempting uninstall: langchain-google-genai\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "    Found existing installation: langchain-google-genai 3.0.0\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "    Uninstalling langchain-google-genai-3.0.0:\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "      Successfully uninstalled langchain-google-genai-3.0.0\n",
      "   -------------------- ------------------- 2/4 [google-generativeai]\n",
      "   ------------------------------ --------- 3/4 [langchain-google-genai]\n",
      "   ------------------------------ --------- 3/4 [langchain-google-genai]\n",
      "   ------------------------------ --------- 3/4 [langchain-google-genai]\n",
      "   ---------------------------------------- 4/4 [langchain-google-genai]\n",
      "\n",
      "Successfully installed google-ai-generativelanguage-0.6.15 google-generativeai-0.8.5 langchain-core-0.3.79 langchain-google-genai-2.0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 1.0.3 requires langchain-core<2.0.0,>=1.0.0, but you have langchain-core 0.3.79 which is incompatible.\n",
      "langchain-classic 1.0.0 requires langchain-core<2.0.0,>=1.0.0, but you have langchain-core 0.3.79 which is incompatible.\n",
      "langchain-community 0.4.1 requires langchain-core<2.0.0,>=1.0.1, but you have langchain-core 0.3.79 which is incompatible.\n",
      "langchain-text-splitters 1.0.0 requires langchain-core<2.0.0,>=1.0.0, but you have langchain-core 0.3.79 which is incompatible.\n",
      "langgraph-prebuilt 1.0.2 requires langchain-core>=1.0.0, but you have langchain-core 0.3.79 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade google-generativeai langchain-core langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec5a5424-b28b-416b-bca6-e8e4679d4768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Install the heavy libraries\n",
    "# !pip install -U pymupdf faiss-cpu sentence-transformers\n",
    "\n",
    "# # 2. Install the new LangChain ecosystem\n",
    "# !pip install -U langchain langchain-community langchain-core langchain-text-splitters\n",
    "\n",
    "# # 3. Install the Google libraries\n",
    "# !pip install -U google-generativeai langchain-google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2233ce-2e68-49ce-a85f-1e642076c5d5",
   "metadata": {},
   "source": [
    "## Step 2: API Key & Model Configuration\n",
    "\n",
    "Set your Google AI Studio API key here. We also define the models we'll use:\n",
    "* `EMBEDDING_MODEL`: A free, local model to turn our documents into vectors.\n",
    "* `GENERATOR_MODEL`: The LLM we will use as our \"fine-tuned\" agent.\n",
    "* `EVALUATOR_MODEL`: A powerful LLM to act as our \"LLM-as-a-Judge\" for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bb74956-c7e5-405e-8816-f00314fb55cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries installed and models configured.\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- CONFIGURATION CELL ---\n",
    "# Get your API key from https://aistudio.google.com/app/apikey\n",
    "os.environ['GOOGLE_API_KEY'] = \"AIzaSyDoqeMHZ_3NvBkabMKk8Kzvlz98oCiPp_k\"\n",
    "\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "GENERATOR_MODEL = \"gemini-2.5-flash-preview-09-2025\"\n",
    "EVALUATOR_MODEL = \"gemini-2.5-flash-preview-09-2025\"\n",
    "# --------------------------\n",
    "\n",
    "try:\n",
    "    genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "    print(\"Libraries installed and models configured.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error configuring API key: {e}\")\n",
    "    print(\"Please paste your GOOGLE_API_KEY in the cell above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5ad1ff-de47-4e3c-bb9b-a135d71acce8",
   "metadata": {},
   "source": [
    "## Step 3: Document Ingestion (The \"RAG\" part)\n",
    "\n",
    "First, we need to load all 14 of your PDF files. Make sure they are in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4c6bb75-743a-419b-ae76-6eb18a4a87b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting document loading...\n",
      "Loading Chapter 4d_Isothermal Reactor Design_Pressure drop_ef34c59e-79c9-419f-9f47-53ce881278a7.pdf...\n",
      "Loading Sixth Week_d360120f-25b2-4ed6-a099-fc545a4caf2c.pdf...\n",
      "Loading Slides_gradient_descent_8c2b629a-0e3d-41cc-b7ca-daa3a4fc05e7.pdf...\n",
      "Loading process-dynamics-and-control-dale-e.-seborg-thomas-f.-edgar-etc.-z-lib.org_.pdf...\n",
      "Loading Week 1 and 2 slides_beb540a4-aa3f-4ad1-aec7-69f18ae8caa4.pdf...\n",
      "Loading Week 3 and 4 slides_cfb83a72-a633-4f37-a935-4ec2f5c01bf0.pdf...\n",
      "Loading Fifth Week slides_618d3ea2-e353-4668-91f9-c1c1c82a29bc.pdf...\n",
      "Loading Chapter 4a_Isothermal Reactor Design_93adbbf0-eb51-4d1b-aa5c-5ce4794060ab.pdf...\n",
      "Loading Chapter 2_9275099c-0e89-4f90-a973-d32f8e0e6d90.pdf...\n",
      "Loading Chapter 2a Problems_d091be5f-1072-4ce3-98ef-4f0c4933a055.pdf...\n",
      "Loading Chapter 3_94d61bbe-18fe-4baa-a9e8-c779baa9607e  -  Read-Only.pdf...\n",
      "Loading Chapter 3b - stochiometry_1d14261e-ed9b-4491-81f8-9845ffbba362.pdf...\n",
      "Loading Chapter 6b_d29c85c9-89d7-4d89-b020-d6dcacfec6ab.pdf...\n",
      "Loading Chapter 6a_1e8a7b02-783a-478b-9561-f39f8ed60387.pdf...\n",
      "\n",
      "Loading complete. Took 7.71 seconds.\n",
      "Loaded 971 pages from 14 files.\n",
      "Split into 2739 text chunks.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import time\n",
    "\n",
    "# --- List of all your PDF files ---\n",
    "pdf_files = [\n",
    "    # Original 4 files\n",
    "    \"Chapter 4d_Isothermal Reactor Design_Pressure drop_ef34c59e-79c9-419f-9f47-53ce881278a7.pdf\",\n",
    "    \"Sixth Week_d360120f-25b2-4ed6-a099-fc545a4caf2c.pdf\",\n",
    "    \"Slides_gradient_descent_8c2b629a-0e3d-41cc-b7ca-daa3a4fc05e7.pdf\",\n",
    "    \"process-dynamics-and-control-dale-e.-seborg-thomas-f.-edgar-etc.-z-lib.org_.pdf\",\n",
    "    \n",
    "    # New 10 files\n",
    "    \"Week 1 and 2 slides_beb540a4-aa3f-4ad1-aec7-69f18ae8caa4.pdf\",\n",
    "    \"Week 3 and 4 slides_cfb83a72-a633-4f37-a935-4ec2f5c01bf0.pdf\",\n",
    "    \"Fifth Week slides_618d3ea2-e353-4668-91f9-c1c1c82a29bc.pdf\",\n",
    "    \"Chapter 4a_Isothermal Reactor Design_93adbbf0-eb51-4d1b-aa5c-5ce4794060ab.pdf\",\n",
    "    \"Chapter 2_9275099c-0e89-4f90-a973-d32f8e0e6d90.pdf\",\n",
    "    \"Chapter 2a Problems_d091be5f-1072-4ce3-98ef-4f0c4933a055.pdf\",\n",
    "    \"Chapter 3_94d61bbe-18fe-4baa-a9e8-c779baa9607e  -  Read-Only.pdf\",\n",
    "    \"Chapter 3b - stochiometry_1d14261e-ed9b-4491-81f8-9845ffbba362.pdf\",\n",
    "    \"Chapter 6b_d29c85c9-89d7-4d89-b020-d6dcacfec6ab.pdf\",\n",
    "    \"Chapter 6a_1e8a7b02-783a-478b-9561-f39f8ed60387.pdf\"\n",
    "]\n",
    "# ----------------------------------\n",
    "\n",
    "all_docs = []\n",
    "print(\"Starting document loading...\")\n",
    "\n",
    "start_time = time.time()\n",
    "for pdf in pdf_files:\n",
    "    if os.path.exists(pdf):\n",
    "        print(f\"Loading {pdf}...\")\n",
    "        try:\n",
    "            loader = PyMuPDFLoader(pdf)\n",
    "            docs = loader.load()\n",
    "            # Add the filename to each document's metadata\n",
    "            for doc in docs:\n",
    "                doc.metadata['source'] = pdf\n",
    "            all_docs.extend(docs)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load {pdf}. Error: {e}\")\n",
    "    else:\n",
    "        print(f\"Warning: File not found - {pdf}\")\n",
    "\n",
    "# Split the documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "all_splits = text_splitter.split_documents(all_docs)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nLoading complete. Took {end_time - start_time:.2f} seconds.\")\n",
    "print(f\"Loaded {len(all_docs)} pages from {len(pdf_files)} files.\")\n",
    "print(f\"Split into {len(all_splits)} text chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086abae0-d715-41a5-a973-847492c33807",
   "metadata": {},
   "source": [
    "## Step 4: Create the Vector Store\n",
    "\n",
    "Now we'll use our embedding model to convert all the text chunks into vectors and store them in a local FAISS database. This database allows us to perform fast similarity searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bca35bab-add1-4506-9723-9779c83fe158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embedding model... (This may take a moment the first time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanis\\AppData\\Local\\Temp\\ipykernel_19240\\313256496.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector store from text chunks...\n",
      "Vector store created. Took 152.03 seconds.\n",
      "Vector store saved to 'my_course_notes_db'\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "print(\"Initializing embedding model... (This may take a moment the first time)\")\n",
    "# This model runs locally on your machine\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "\n",
    "print(\"Creating vector store from text chunks...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create the FAISS vector store from our document chunks\n",
    "# This will take a few minutes as it processes all 14 documents\n",
    "db = FAISS.from_documents(all_splits, embeddings)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Vector store created. Took {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# Save the vector store locally (optional, but good practice)\n",
    "db.save_local(\"my_course_notes_db\")\n",
    "print(\"Vector store saved to 'my_course_notes_db'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc29a04-d5a2-400f-9945-d82caa182979",
   "metadata": {},
   "source": [
    "## Step 5: The Fine-Tuning Target (Mandatory Requirement)\n",
    "\n",
    "To meet the assignment's **mandatory** fine-tuning requirement, we will target the **Generator Agent**. Our goal is to train a model that acts as an **expert tutor** based *only* on our documents.\n",
    "\n",
    "To do this, we must create a dataset. This is a *key deliverable* for your data science report. Here is a high-quality dataset based on the new PDFs you uploaded.\n",
    "\n",
    "This file (`tutor_finetuning_data.jsonl`) is what you would submit to the Gemini API's fine-tuning service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69306da8-c165-4ff7-a883-8f699f532543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tutor_finetuning_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%writefile tutor_finetuning_data.jsonl\n",
    "{\"input\": {\"question\": \"What is the design equation for a CSTR in terms of conversion?\", \"context\": \"CSTR... V = (F_A0 * X) / (-r_A). CSTR volume necessary to achieve conversion X.\"}, \"output\": {\"answer\": \"According to 'Chapter 2_9275099c-0e89-4f90-a973-d32f8e0e6d90.pdf', the design equation for a CSTR (Continuous Stirred-Tank Reactor) in terms of conversion (X) is:\\n\\n`V = (F_A0 * X) / (-r_A)`\\n\\nwhere V is the CSTR volume, F_A0 is the molar flow rate of A fed, X is the conversion of A, and -r_A is the rate of disappearance of A evaluated at the exit conditions.\"}}\n",
    "{\"input\": {\"question\": \"What is a nonelementary rate law?\", \"context\": \"Nonelementary Rate Laws: A large number of both homogeneous and heterogeneous reactions do not follow simple rate laws... For example... CO + Cl2 -> COCl2 ... the kinetic rate law is -r_CO = k * C_CO * C_Cl2^(3/2)\"}, \"output\": {\"answer\": \"Based on 'Chapter 3_94d61bbe-18fe-4baa-a9e8-c779baa9607e - Read-Only.pdf', a nonelementary rate law is one that does not follow a simple rate law. The reaction orders do not necessarily agree with the stoichiometric coefficients. For example, the reaction `CO + Cl2 -> COCl2` has a rate law of `-r_CO = k * C_CO * C_Cl2^(3/2)`, which is not elementary.\"}}\n",
    "{\"input\": {\"question\": \"What is the difference between parallel and series reactions?\", \"context\": \"Parallel reactions: (also called competing reactions) Reactions where the reactant is consumed by two different reaction pathways... Series reaction: (also called consecutive reactions) Reactions where the reactant forms an intermediate product, which reacts further to form another product: A -> B -> C\"}, \"output\": {\"answer\": \"According to 'Chapter 6a_1e8a7b02-783a-478b-9561-f39f8ed60387.pdf':\\n* **Parallel reactions** (or competing reactions) are when a reactant is consumed by two or more different pathways to form different products (e.g., A -> B and A -> C).\\n* **Series reactions** (or consecutive reactions) are when a reactant forms an intermediate product, which then reacts further to form another product (e.g., A -> B -> C).\"}}\n",
    "{\"input\": {\"question\": \"What is the 'myth of technological value neutrality'?\", \"context\": \"The Value Neutrality Thesis in Al... Claim: Technology itself is morally and politically neutral; only its uses have value... Problem: ignores value-laden choices inside the design process.\"}, \"output\": {\"answer\": \"Based on 'Week 3 and 4 slides_cfb83a72-a633-4f37-a935-4ec2f5c01bf0.pdf', the 'myth of technological value neutrality' is the claim that technology (like AI) is just a \"}}\n",
    "{\"input\": {\"question\": \"What is the 'Pareto Curve' in the context of fairness?\", \"context\": \"The Pareto curve of accuracy and fairness... a curve connecting the set of undominated models... any model that is not on this boundary is a 'bad' model... The Pareto curve... is necessarily silent about which point we should choose... that is a matter of judgment\"}, \"output\": {\"answer\": \"According to 'Fifth Week slides_618d3ea2-e353-4668-91f9-c1c1c82a29bc.pdf', the 'Pareto Curve' in the context of fairness plots 'unfairness' vs. 'error' for different models. It connects the set of 'undominated' models, where you cannot improve one metric (e.g., fairness) without harming the other (e.g., accuracy). It shows the trade-offs, but the final choice of which model to use from the curve is a 'matter of judgment'.\"}}\n",
    "{\"input\": {\"question\": \"What is the capital of Argentina?\", \"context\": \"Chapter 2: Conversion and Reactor Sizing... A+B -> C+D... CSTR: V = (F_A0 * X) / (-r_A)\"}, \"output\": {\"answer\": \"I'm sorry, but I could not find that information in your documents.\"}}\n",
    "{\"input\": {\"question\": \"What is conversion, X?\", \"context\": \"Definition of conversion, X (measure of reaction's progress)... X = (moles A reacted) / (moles A fed)\"}, \"output\": {\"answer\": \"According to 'Chapter 2_9275099c-0e89-4f90-a973-d32f8e0e6d90.pdf', conversion (X) is a measure of a reaction's progress. It is defined as:\\n\\n`X = (moles A reacted) / (moles A fed)`\\n\\nwhere A is the limiting reactant.\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6361ddf9-1122-4ddc-ad8e-563d9eb872b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning dataset 'tutor_finetuning_data.jsonl' created.\n"
     ]
    }
   ],
   "source": [
    "print(\"Fine-tuning dataset 'tutor_finetuning_data.jsonl' created.\")\n",
    "# You would now upload this file to the Gemini API to train your custom model.\n",
    "# For this prototype, we will *simulate* this fine-tuned model in the next step\n",
    "# by using a strong system prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf492024-cb46-481a-b269-1a67798b9d6f",
   "metadata": {},
   "source": [
    "## Step 6: Simulate the Fine-Tuned Agent (Generator & RAG)\n",
    "\n",
    "This is the main \"ask\" function. It implements the full RAG pipeline:\n",
    "1.  Takes a `query`.\n",
    "2.  Searches the `db` (vector store) for the 4 most relevant document chunks.\n",
    "3.  Combines these chunks into a `context`.\n",
    "4.  Sends the `query` and `context` to our `GENERATOR_MODEL`.\n",
    "5.  **Crucially**, it uses a `system_instruction` to *simulate* our fine-tuned tutor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8276d0b9-513e-4a27-9169-fb96ac19af2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.globals import set_verbose, set_debug, set_llm_cache\n",
    "\n",
    "# Set the global verbosity, debug, and cache flags\n",
    "# This prevents all the internal AttributeErrors\n",
    "set_verbose(False) \n",
    "set_debug(False)\n",
    "set_llm_cache(None) # <-- ADD THIS LINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67ee3aaa-a0fd-47e2-9620-42b9afbadff4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What is conversion, X?\n",
      "Agent is thinking...\n",
      "Agent response (took 4.24s):\n",
      "\n",
      "Conversion, X, is defined as a measure of a reaction's progress, quantifying how far a reaction proceeds to the right.\n",
      "\n",
      "If the limiting reactant A is chosen as the basis of calculation, conversion X is defined by the ratio:\n",
      "\n",
      "$$X = \\frac{\\text{moles A reacted}}{\\text{moles A fed}}$$\n",
      "\n",
      "For reversible reactions, the maximum conversion achievable is the equilibrium conversion.\n",
      "\n",
      "Conversion is a function of the time the reactants spend in the reactor (for batch reactors) or a function of the reactor volume V (for continuous-flow systems).\n",
      "\n",
      "Source: Chapter 2_9275099c-0e89-4f90-a973-d32f8e0e6d90.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Conversion, X, is defined as a measure of a reaction's progress, quantifying how far a reaction proceeds to the right.\\n\\nIf the limiting reactant A is chosen as the basis of calculation, conversion X is defined by the ratio:\\n\\n$$X = \\\\frac{\\\\text{moles A reacted}}{\\\\text{moles A fed}}$$\\n\\nFor reversible reactions, the maximum conversion achievable is the equilibrium conversion.\\n\\nConversion is a function of the time the reactants spend in the reactor (for batch reactors) or a function of the reactor volume V (for continuous-flow systems).\\n\\nSource: Chapter 2_9275099c-0e89-4f90-a973-d32f8e0e6d90.pdf\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# --- CORRECTED IMPORT ---\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "# --- CORRECTED IMPORT ---\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import re\n",
    "import time  # --- ADDED IMPORT ---\n",
    "\n",
    "# This is the prompt that *simulates* our fine-tuned model.\n",
    "# This is the most important part of our \"Generator Agent.\"\n",
    "SYSTEM_INSTRUCTION = \"\"\"You are an expert academic tutor. Your specialty is Chemical Engineering and AI Ethics, based on the user's provided notes.\n",
    "A user will provide you with a QUESTION and a CONTEXT from their course notes.\n",
    "Your task is to answer the user's QUESTION based *only* on the provided CONTEXT.\n",
    "\n",
    "Follow these rules strictly:\n",
    "1.  **Base your answer ONLY on the CONTEXT.** Do not use any outside knowledge.\n",
    "2.  If the CONTEXT does not contain the answer, you MUST state: \"I'm sorry, but I could not find that information in your documents.\" Do not try to answer it from memory.\n",
    "3.  **Cite your sources.** At the end of your answer, clearly state which document(s) you used, referencing the 'source' metadata from the CONTEXT (e.g., \"Source: Chapter 2_...pdf\").\n",
    "4.  Format formulas or code snippets clearly using markdown (e.g., `G(s) = Y(s) / U(s)`).\n",
    "5.  Be concise and answer as a helpful tutor.\n",
    "\"\"\"\n",
    "\n",
    "# Create the Generator LLM\n",
    "# We removed cache=False here, as the global set_llm_cache(None) fixed the root problem\n",
    "llm = ChatGoogleGenerativeAI(model=GENERATOR_MODEL,\n",
    "                             temperature=0,\n",
    "                             convert_system_message_to_human=True) # Important for models that prefer user/model turns\n",
    "\n",
    "# Create a \"retriever\" from our vector database\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 4}) # k=4 means it will find the 4 best chunks\n",
    "\n",
    "def format_docs(docs):\n",
    "    # Combine the document chunks into a single string for the prompt\n",
    "    return \"\\n\\n---\\n\\n\".join(\n",
    "        f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\\n{doc.page_content}\" for doc in docs\n",
    "    )\n",
    "\n",
    "# Build the RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | ChatPromptTemplate.from_messages([\n",
    "        (\"system\", SYSTEM_INSTRUCTION),\n",
    "        (\"human\", \"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{question}\")\n",
    "    ])\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "def ask_agent(query):\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"Agent is thinking...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # Invoke the full RAG chain\n",
    "    answer = rag_chain.invoke(query)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Agent response (took {end_time - start_time:.2f}s):\\n\")\n",
    "    print(answer)\n",
    "    return answer\n",
    "\n",
    "# --- Test the Agent ---\n",
    "ask_agent(\"What is conversion, X?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae8ea57-dd27-4e62-aefe-9fe7e31e9612",
   "metadata": {},
   "source": [
    "## Step 7: Evaluation (Mandatory Requirement)\n",
    "\n",
    "Now we build our evaluation system. This is the **second mandatory part** of your assignment.\n",
    "\n",
    "We will create an \"LLM-as-a-Judge\" to rate our agent's answers on two key metrics:\n",
    "1.  **Groundedness:** Is the answer fully based on the provided context? (1-5 scale)\n",
    "2.  **Helpfulness:** Does the answer directly and correctly answer the question? (1-5 scale)\n",
    "\n",
    "We will run a test suite of questions and get a quantitative score for our data science report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28ad50eb-bd96-4899-842f-e4c487002253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING AGENT EVALUATION ---\n",
      "\n",
      "[Test 1/4] Question: What is the design equation for a CSTR in terms of conversion?\n",
      "Answer: The design equation for the CSTR volume (V) necessary to achieve conversion X is:\n",
      "\n",
      "`V = FA0 * X / (rA)`\n",
      "\n",
      "This equation is derived from the relationship:\n",
      "\n",
      "`V = (FA0  FA) / (rA)`\n",
      "\n",
      "where `FA0` is the moles entering A, `FA` is the moles leaving A, and the rate of reaction (`rA`) is evaluated at the exit conditions, which are identical to the composition inside the reactor.\n",
      "\n",
      "Source: Chapter 2_9275099c-0e89-4f90-a973-d32f8e0e6d90.pdf\n",
      "Evaluation: {'groundedness_score': 5, 'groundedness_reason': 'The answer accurately states the design equation V = FA0 * X / (rA) and the related variables and conditions directly from the context provided in the second source snippet.', 'helpfulness_score': 5, 'helpfulness_reason': 'The answer directly provides the requested design equation for a CSTR in terms of conversion (X) and clarifies the meaning of the variables involved, making it highly helpful and complete.'}\n",
      "\n",
      "[Test 2/4] Question: What is the difference between parallel and series reactions?\n",
      "Answer: Based on your notes, the difference between parallel and series reactions relates to the pathway the reactant takes to form products:\n",
      "\n",
      "*   **Parallel reactions** (also called competing reactions) are reactions where the reactant is consumed by two different reaction pathways to form different products.\n",
      "*   **Series reactions** (also called consecutive reactions) are reactions where the reactant forms an intermediate product, which then reacts further to form another product.\n",
      "\n",
      "Source: Chapter 6a_1e8a7b02-783a-478b-9561-f39f8ed60387.pdf\n",
      "Evaluation: {'groundedness_score': 5, 'groundedness_reason': \"The answer is fully grounded as it directly quotes the context's definitions for both parallel (competing) and series (consecutive) reactions, accurately explaining the difference in reaction pathways.\", 'helpfulness_score': 5, 'helpfulness_reason': 'The answer is highly helpful, providing a clear and complete explanation of the difference between parallel and series reactions by defining the unique pathway of each type.'}\n",
      "\n",
      "[Test 3/4] Question: What is 'algocracy'?\n",
      "Answer: Algocracy refers to a system where algorithms are used to collect, collate, and organize the data upon which decisions are typically made, and to assist in how that data is processed and communicated through the relevant governance system.\n",
      "\n",
      "In an algocratic system:\n",
      "*   Algorithms structure and constrain the ways in which humans interact with one another, the relevant data, and the broader community affected by those systems.\n",
      "*   This structuring can occur by algorithms packaging and organizing information in a specific way or by forcing changes in the structure of the physical environment where humans operate.\n",
      "*   Increasing reliance on algocratic systems limits the scope for active human participation in and comprehension of decision-making procedures, which is considered a threat to legitimate decision-making.\n",
      "*   In such systems, algorithms may perform the \"moral calculus on their own,\" leading to social spaces that are opaque to human reason.\n",
      "\n",
      "Source: Sixth Week_d360120f-25b2-4ed6-a099-fc545a4caf2c.pdf\n",
      "Evaluation: {'groundedness_score': 5, 'groundedness_reason': 'All information provided in the answer, including the definition and the four bullet points describing the system, is directly supported by the first two provided context blocks.', 'helpfulness_score': 5, 'helpfulness_reason': \"The answer provides a clear and comprehensive definition of 'algocracy' by summarizing its function, its effect on human interaction, and its implications for decision-making procedures, fully addressing the user's question using the provided source material.\"}\n",
      "\n",
      "[Test 4/4] Question: What is the capital of Argentina?\n",
      "Answer: I'm sorry, but I could not find that information in your documents.\n",
      "Evaluation: {'groundedness_score': 5, 'groundedness_reason': 'The answer correctly states that the requested information (capital of Argentina) is not present in the provided context, which deals with regression models and genetics.', 'helpfulness_score': 4, 'helpfulness_reason': \"The answer is helpful by honestly reporting that the information could not be found, managing user expectations. It doesn't receive a 5 because it fails to provide the factual answer the user sought, although this limitation is due to the context provided.\"}\n",
      "\n",
      "--- EVALUATION COMPLETE ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "import json  # <-- Make sure json is imported, it's used later\n",
    "\n",
    "# Define the structured output we want from the evaluator\n",
    "EVALUATION_SCHEMA = {\n",
    "    \"type\": \"OBJECT\",\n",
    "    \"properties\": {\n",
    "        \"groundedness_score\": {\n",
    "            \"type\": \"INTEGER\",\n",
    "            \"description\": \"Score from 1-5 for how well the answer is based *only* on the context. 1=Hallucinated, 5=Fully Grounded.\"\n",
    "        },\n",
    "        \"groundedness_reason\": {\n",
    "            \"type\": \"STRING\",\n",
    "            \"description\": \"A brief reason for the groundedness score.\"\n",
    "        },\n",
    "        \"helpfulness_score\": {\n",
    "            \"type\": \"INTEGER\",\n",
    "            \"description\": \"Score from 1-5 for how well the answer addresses the user's question. 1=Not helpful, 5=Very helpful.\"\n",
    "        },\n",
    "        \"helpfulness_reason\": {\n",
    "            \"type\": \"STRING\",\n",
    "            \"description\": \"A brief reason for the helpfulness score.\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"groundedness_score\", \"groundedness_reason\", \"helpfulness_score\", \"helpfulness_reason\"]\n",
    "}\n",
    "\n",
    "# Create the Evaluator LLM\n",
    "evaluator_llm = genai.GenerativeModel(\n",
    "    EVALUATOR_MODEL,\n",
    "    system_instruction=\"You are a strict and fair evaluator for a RAG (Retrieval-Augmented Generation) system. You will be given a QUESTION, the CONTEXT used to answer it, and the generated ANSWER. Your job is to score the ANSWER on 'groundedness' and 'helpfulness' based on the provided schema.\",\n",
    ")\n",
    "\n",
    "# --- MOVED THIS FUNCTION TO THE TOP ---\n",
    "def exponential_backoff(func):\n",
    "    \"\"\"A decorator for simple exponential backoff.\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        retries = 5\n",
    "        delay = 1.0\n",
    "        for i in range(retries):\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "            except Exception as e:\n",
    "                if i == retries - 1:\n",
    "                    # Do not log retries as errors, per instructions\n",
    "                    # print(f\"Final attempt failed. Error: {e}\")\n",
    "                    raise\n",
    "                time.sleep(delay)\n",
    "                delay = min(delay * 2, 30.0)\n",
    "    return wrapper\n",
    "    \n",
    "@exponential_backoff\n",
    "def evaluate_answer(question, context, answer):\n",
    "    \"\"\"Uses an LLM-as-a-Judge to evaluate the agent's response.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    QUESTION:\n",
    "    {question}\n",
    "\n",
    "    CONTEXT:\n",
    "    {context}\n",
    "\n",
    "    ANSWER:\n",
    "    {answer}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = evaluator_llm.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                response_mime_type=\"application/json\",\n",
    "                response_schema=EVALUATION_SCHEMA\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Clean the JSON response just in case\n",
    "        response_text = response.text\n",
    "        json_start = response_text.find('{')\n",
    "        json_end = response_text.rfind('}')\n",
    "        \n",
    "        if json_start != -1 and json_end != -1:\n",
    "            clean_json = response_text[json_start:json_end+1]\n",
    "            return json.loads(clean_json)\n",
    "        else:\n",
    "            print(f\"Error: Could not find valid JSON in evaluator response: {response_text}\")\n",
    "            return {\"error\": \"Failed to parse evaluation JSON\", \"raw\": response_text, \"groundedness_score\": 0, \"groundedness_reason\": \"Parse fail\", \"helpfulness_score\": 0, \"helpfulness_reason\": \"Parse fail\"}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {e}\")\n",
    "        return {\"error\": str(e), \"groundedness_score\": 0, \"groundedness_reason\": str(e), \"helpfulness_score\": 0, \"helpfulness_reason\": str(e)}\n",
    "\n",
    "# --- Evaluation Test Suite ---\n",
    "test_questions = [\n",
    "    \"What is the design equation for a CSTR in terms of conversion?\",\n",
    "    \"What is the difference between parallel and series reactions?\",\n",
    "    \"What is 'algocracy'?\",\n",
    "    \"What is the capital of Argentina?\" # Test for hallucination\n",
    "]\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "print(\"--- STARTING AGENT EVALUATION ---\")\n",
    "# Re-create retriever and chain to use them inside this loop\n",
    "# NEW, CORRECT LINE\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
    "llm_chain_for_eval = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | ChatPromptTemplate.from_messages([\n",
    "        (\"system\", SYSTEM_INSTRUCTION),\n",
    "        (\"human\", \"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{question}\")\n",
    "    ])\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "# --- CORRECTED FOR LOOP FOR YOUR EVALUATION CELL ---\n",
    "\n",
    "for i, question in enumerate(test_questions):\n",
    "    print(f\"\\n[Test {i+1}/{len(test_questions)}] Question: {question}\")\n",
    "    \n",
    "    # 1. Get the context\n",
    "    # We retrieve the docs here *only* to pass them to the evaluator later.\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    context_str = format_docs(retrieved_docs)\n",
    "    \n",
    "    # 2. Get the agent's answer\n",
    "    # The chain is invoked with ONLY the question string.\n",
    "    # It will run its own retriever internally.\n",
    "    answer = llm_chain_for_eval.invoke(question)  # <-- THIS IS THE FIX\n",
    "    print(f\"Answer: {answer}\")\n",
    "    \n",
    "    # 3. Evaluate the answer\n",
    "    # We pass the question, the answer, and the context we retrieved in step 1\n",
    "    evaluation = evaluate_answer(question, context_str, answer)\n",
    "    print(f\"Evaluation: {evaluation}\")\n",
    "    \n",
    "    result = {\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "    }\n",
    "    result.update(evaluation)\n",
    "    evaluation_results.append(result)\n",
    "    \n",
    "print(\"\\n--- EVALUATION COMPLETE ---\")\n",
    "# Create a DataFrame for easy viewing\n",
    "df_results = pd.DataFrame(evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5362c324-98e2-4eb4-877d-e70852523356",
   "metadata": {},
   "source": [
    "## Step 8: Final Report & Live Demo\n",
    "\n",
    "This cell provides the quantitative results for your Data Science report and a live \"chat\" box to interact with your new agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77e22f7e-33b7-4589-b9f2-c1f23967a257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Data Science Report: Evaluation ======\n",
      "Total Questions Tested: 4\n",
      "Average Groundedness Score: 5.00 / 5.0\n",
      "Average Helpfulness Score: 4.75 / 5.0\n",
      "\n",
      "--- Detailed Results ---\n",
      "Q: What is the design equation for a CSTR in terms of conversion?\n",
      "  G: 5 (The answer accurately states the design equation V = FA0 * X / (rA) and the related variables and conditions directly from the context provided in the second source snippet.)\n",
      "  H: 5 (The answer directly provides the requested design equation for a CSTR in terms of conversion (X) and clarifies the meaning of the variables involved, making it highly helpful and complete.)\n",
      "Q: What is the difference between parallel and series reactions?\n",
      "  G: 5 (The answer is fully grounded as it directly quotes the context's definitions for both parallel (competing) and series (consecutive) reactions, accurately explaining the difference in reaction pathways.)\n",
      "  H: 5 (The answer is highly helpful, providing a clear and complete explanation of the difference between parallel and series reactions by defining the unique pathway of each type.)\n",
      "Q: What is 'algocracy'?\n",
      "  G: 5 (All information provided in the answer, including the definition and the four bullet points describing the system, is directly supported by the first two provided context blocks.)\n",
      "  H: 5 (The answer provides a clear and comprehensive definition of 'algocracy' by summarizing its function, its effect on human interaction, and its implications for decision-making procedures, fully addressing the user's question using the provided source material.)\n",
      "Q: What is the capital of Argentina?\n",
      "  G: 5 (The answer correctly states that the requested information (capital of Argentina) is not present in the provided context, which deals with regression models and genetics.)\n",
      "  H: 4 (The answer is helpful by honestly reporting that the information could not be found, managing user expectations. It doesn't receive a 5 because it fails to provide the factual answer the user sought, although this limitation is due to the context provided.)\n",
      "===========================================\n",
      "\n",
      "--- Live Agent Demo ---\n",
      "Type 'quit' or 'exit' to stop.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask your question:  explain gradient desent\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: explain gradient desent\n",
      "Agent is thinking...\n",
      "Agent response (took 4.19s):\n",
      "\n",
      "Gradient descent is a method used to train Machine Learning (ML) models in Chemical Engineering (Source: Slides_gradient_descent...pdf). Specifically, it is utilized for loss minimization (Source: Slides_gradient_descent...pdf).\n",
      "\n",
      "The process of loss minimization via gradient descent follows these steps:\n",
      "\n",
      "1.  **Initialize** the parameters ($\\Theta$).\n",
      "2.  **Calculate** the loss function $L(\\Theta)$ and its gradient $\\nabla L(\\Theta)$.\n",
      "3.  **Step** in the direction of the negative gradient, $-\\nabla L(\\Theta)$.\n",
      "4.  **Check for Convergence:** The process repeats until a convergence criterion is met, such as $||\\nabla L(\\Theta)|| < \\text{tolerance}$.\n",
      "5.  **Output:** If converged, the learned function $h(x, \\Theta^*)$ is obtained using the optimal parameters $\\Theta^*$ (Source: Slides_gradient_descent...pdf).\n",
      "\n",
      "A key component of this method is calculating the gradient of the loss function (Source: Slides_gradient_descent...pdf).\n",
      "\n",
      "Source: Slides_gradient_descent_8c2b629a-0e3d-41cc-b7ca-daa3a4fc05e7.pdf\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask your question:  formula for conversion in cstr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: formula for conversion in cstr\n",
      "Agent is thinking...\n",
      "Agent response (took 6.05s):\n",
      "\n",
      "The context provides the design equation for the CSTR volume (`V`) necessary to achieve a specified conversion (`X`). While the formula does not isolate `X`, it defines the relationship between volume, molar flow rate, reaction rate, and conversion.\n",
      "\n",
      "The CSTR volume equation is given as:\n",
      "\n",
      "`V = FA0  FA 0 FA0X / (rA)`\n",
      "\n",
      "This is also presented in the context as:\n",
      "\n",
      "`V = FA0 * X / (rA)`\n",
      "\n",
      "The context also defines the relationship for the moles reacted in terms of conversion:\n",
      "\n",
      "`Moles A reacted = FA0 * X`\n",
      "\n",
      "Source: Chapter 2_9275099c-0e89-4f90-a973-d32f8e0e6d90.pdf\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask your question:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent shutting down. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Final Report ---\n",
    "print(\"====== Data Science Report: Evaluation ======\")\n",
    "if 'df_results' in locals() and not df_results.empty:\n",
    "    # Filter out potential error rows before calculating mean\n",
    "    valid_results = df_results[pd.to_numeric(df_results['groundedness_score'], errors='coerce').notna()]\n",
    "    avg_groundedness = valid_results['groundedness_score'].mean()\n",
    "    avg_helpfulness = valid_results['helpfulness_score'].mean()\n",
    "    \n",
    "    print(f\"Total Questions Tested: {len(df_results)}\")\n",
    "    print(f\"Average Groundedness Score: {avg_groundedness:.2f} / 5.0\")\n",
    "    print(f\"Average Helpfulness Score: {avg_helpfulness:.2f} / 5.0\")\n",
    "    \n",
    "    print(\"\\n--- Detailed Results ---\")\n",
    "    for index, row in df_results.iterrows():\n",
    "        print(f\"Q: {row['question']}\")\n",
    "        print(f\"  G: {row['groundedness_score']} ({row['groundedness_reason']})\")\n",
    "        print(f\"  H: {row['helpfulness_score']} ({row['helpfulness_reason']})\")\n",
    "else:\n",
    "    print(\"Evaluation results are empty. Please run Step 7.\")\n",
    "    \n",
    "print(\"===========================================\")\n",
    "\n",
    "\n",
    "# --- 2. Live Demo (Bonus: User Interface) ---\n",
    "print(\"\\n--- Live Agent Demo ---\")\n",
    "print(\"Type 'quit' or 'exit' to stop.\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        query = input(\"\\nAsk your question: \")\n",
    "        if query.lower() in ['quit', 'exit']:\n",
    "            print(\"Agent shutting down. Goodbye!\")\n",
    "            break\n",
    "        # We can just use the function from Cell 14\n",
    "        ask_agent(query)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nAgent shut down.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d262e68-5a99-4379-b55f-da920046e6d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7cdeea-e057-4ba1-8521-f38e36acf75a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3c187f-254c-4944-b8c0-f7efae86ac52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923ccf77-6f73-4558-afb2-889a6fa606a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
